{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q learning with Doom üïπÔ∏è\n",
    "In this notebook we'll implement an agent <b>that plays Doom by using a Deep Q learning architecture.</b> <br>\n",
    "Our agent playing Doom:\n",
    "\n",
    "<img src=\"assets/doom.gif\" style=\"max-width: 600px;\" alt=\"Deep Q learning with Doom\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
    "- Deep Q-Learning [Article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n",
    "- In the [video version](https://www.youtube.com/watch?v=gCJyVX98KJ4)  we implemented a Deep Q-learning agent with Tensorflow that learns to play Atari Space Invaders üïπÔ∏èüëæ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amrmkayid/anaconda3/envs/kayddrl/lib/python3.7/site-packages/IPython/core/display.py:689: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gCJyVX98KJ4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gCJyVX98KJ4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf      # Deep Learning library\n",
    "import numpy as np           # Handle matrices\n",
    "from vizdoom import *        # Doom Environment\n",
    "\n",
    "import random                # Handling random number generation\n",
    "import time                  # Handling time calculation\n",
    "from skimage import transform# Help us to preprocess the frames\n",
    "\n",
    "from collections import deque# Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out. \n",
    "\n",
    "### Our environment\n",
    "<img src=\"assets/doom.png\" style=\"max-width:500px;\" alt=\"Doom\"/>\n",
    "                                    \n",
    "- A monster is spawned **randomly somewhere along the opposite wall**. \n",
    "- Player can only go **left/right and shoot**. \n",
    "- 1 hit is enough **to kill the monster**. \n",
    "- Episode finishes when **monster is killed or on timeout (300)**.\n",
    "<br><br>\n",
    "REWARDS:\n",
    "\n",
    "- +101 for killing the monster \n",
    "- -5 for missing \n",
    "- Episode ends after killing the monster or on timeout.\n",
    "- living reward = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case basic scenario)\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here our possible actions\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "       \n",
    "\"\"\"\n",
    "Here we performing random action to test the environment\n",
    "\"\"\"\n",
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    shoot = [0, 0, 1]\n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    actions = [shoot, left, right]\n",
    "\n",
    "    episodes = 10\n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state()\n",
    "            img = state.screen_buffer\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            print (\"\\treward:\", reward)\n",
    "            time.sleep(0.02)\n",
    "        print (\"Result:\", game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess_frame:\n",
    "    Take a frame.\n",
    "    Resize it.\n",
    "        __________________\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |                 |\n",
    "        |_________________|\n",
    "        \n",
    "        to\n",
    "        _____________\n",
    "        |            |\n",
    "        |            |\n",
    "        |            |\n",
    "        |____________|\n",
    "    Normalize it.\n",
    "    \n",
    "    return preprocessed_frame\n",
    "    \n",
    "    \"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame already done in our vizdoom config\n",
    "    # x = np.mean(frame,-1)\n",
    "    \n",
    "    # Crop the screen (remove the roof because it contains no information)\n",
    "    cropped_frame = frame[30:-10,30:-30]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "        \n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \n",
    "    \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Deep Q-learning Neural Network model üß†\n",
    "<img src=\"assets/model.png\" alt=\"Model\" />\n",
    "This is our Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Finally it passes through 2 FC layers\n",
    "- It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 84, 84, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 84x84x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            ## --> [20, 20, 32]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            ## --> [9, 9, 64]\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            BatchNormalization\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            ## --> [3, 3, 128]\n",
    "            \n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            ## --> [1152]\n",
    "            \n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"fc1\")\n",
    "            \n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = 3, \n",
    "                                        activation=None)\n",
    "\n",
    "  \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Experience Replay method.** <br><br>\n",
    "Here we'll create the Memory object that creates a deque.A deque (double ended queue) is a data type that **removes the oldest element each time that you add a new element.**\n",
    "\n",
    "This part was taken from Udacity : <a href=\"https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb\" Cartpole DQN</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience (state, action, reward, new_state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "    \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    # If we're dead\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Our state is now the next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Set up Tensorboard üìä\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dqn/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"./doom_dqn_summaries\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 2 Total reward: 95.0 Training loss: 170.9753 Explore P: 0.9798\n",
      "Model Saved\n",
      "Model Saved\n",
      "Episode: 14 Total reward: 69.0 Training loss: 0.2811 Explore P: 0.8765\n",
      "Model Saved\n",
      "Episode: 16 Total reward: 90.0 Training loss: 5.3749 Explore P: 0.8669\n",
      "Model Saved\n",
      "Model Saved\n",
      "Model Saved\n",
      "Episode: 31 Total reward: 84.0 Training loss: 0.1665 Explore P: 0.7537\n",
      "Episode: 32 Total reward: 92.0 Training loss: 0.8581 Explore P: 0.7530\n",
      "Episode: 35 Total reward: 73.0 Training loss: 0.7216 Explore P: 0.7366\n",
      "Model Saved\n",
      "Episode: 39 Total reward: 93.0 Training loss: 0.9828 Explore P: 0.7146\n",
      "Model Saved\n",
      "Episode: 45 Total reward: 94.0 Training loss: 0.3080 Explore P: 0.6798\n",
      "Model Saved\n",
      "Episode: 47 Total reward: 16.0 Training loss: 0.2692 Explore P: 0.6685\n",
      "Episode: 48 Total reward: 95.0 Training loss: 0.5137 Explore P: 0.6681\n",
      "Episode: 50 Total reward: 88.0 Training loss: 1.5366 Explore P: 0.6607\n",
      "Model Saved\n",
      "Episode: 55 Total reward: 45.0 Training loss: 1.5316 Explore P: 0.6323\n",
      "Model Saved\n",
      "Model Saved\n",
      "Episode: 62 Total reward: 50.0 Training loss: 1.9417 Explore P: 0.5937\n",
      "Episode: 63 Total reward: 52.0 Training loss: 2.8713 Explore P: 0.5914\n",
      "Episode: 65 Total reward: 94.0 Training loss: 3.3291 Explore P: 0.5852\n",
      "Model Saved\n",
      "Episode: 68 Total reward: 59.0 Training loss: 13.8146 Explore P: 0.5717\n",
      "Model Saved\n",
      "Episode: 71 Total reward: 85.0 Training loss: 1.3502 Explore P: 0.5597\n",
      "Episode: 73 Total reward: 94.0 Training loss: 2.1141 Explore P: 0.5539\n",
      "Model Saved\n",
      "Episode: 76 Total reward: 95.0 Training loss: 1.9972 Explore P: 0.5428\n",
      "Episode: 77 Total reward: -13.0 Training loss: 1.5374 Explore P: 0.5378\n",
      "Model Saved\n",
      "Episode: 83 Total reward: 64.0 Training loss: 5.2147 Explore P: 0.5104\n",
      "Episode: 84 Total reward: 52.0 Training loss: 2.9253 Explore P: 0.5085\n",
      "Model Saved\n",
      "Episode: 87 Total reward: 31.0 Training loss: 1.7034 Explore P: 0.4957\n",
      "Episode: 88 Total reward: 12.0 Training loss: 2.5291 Explore P: 0.4921\n",
      "Episode: 89 Total reward: 13.0 Training loss: 1.7365 Explore P: 0.4886\n",
      "Episode: 90 Total reward: 88.0 Training loss: 2.0101 Explore P: 0.4880\n",
      "Model Saved\n",
      "Episode: 91 Total reward: 61.0 Training loss: 3.0401 Explore P: 0.4863\n",
      "Episode: 94 Total reward: 48.0 Training loss: 3.6260 Explore P: 0.4749\n",
      "Episode: 95 Total reward: 49.0 Training loss: 9.4013 Explore P: 0.4729\n",
      "Model Saved\n",
      "Episode: 98 Total reward: 38.0 Training loss: 3.4764 Explore P: 0.4614\n",
      "Episode: 99 Total reward: 36.0 Training loss: 2.4334 Explore P: 0.4589\n",
      "Episode: 100 Total reward: 93.0 Training loss: 2.7800 Explore P: 0.4585\n",
      "Model Saved\n",
      "Episode: 102 Total reward: 36.0 Training loss: 2.7366 Explore P: 0.4516\n",
      "Episode: 103 Total reward: 50.0 Training loss: 3.2126 Explore P: 0.4498\n",
      "Episode: 105 Total reward: 42.0 Training loss: 2.5049 Explore P: 0.4433\n",
      "Model Saved\n",
      "Episode: 106 Total reward: 27.0 Training loss: 5.7390 Explore P: 0.4408\n",
      "Episode: 107 Total reward: 85.0 Training loss: 5.6326 Explore P: 0.4401\n",
      "Episode: 108 Total reward: 95.0 Training loss: 2.1630 Explore P: 0.4398\n",
      "Episode: 109 Total reward: 70.0 Training loss: 3.7957 Explore P: 0.4387\n",
      "Episode: 110 Total reward: 71.0 Training loss: 5.9090 Explore P: 0.4376\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 25.0 Training loss: 6.5410 Explore P: 0.4348\n",
      "Episode: 112 Total reward: 45.0 Training loss: 8.7434 Explore P: 0.4327\n",
      "Episode: 113 Total reward: -19.0 Training loss: 4.5109 Explore P: 0.4285\n",
      "Episode: 114 Total reward: 32.0 Training loss: 12.8469 Explore P: 0.4262\n",
      "Episode: 115 Total reward: 55.0 Training loss: 5.2768 Explore P: 0.4247\n",
      "Model Saved\n",
      "Episode: 117 Total reward: 92.0 Training loss: 5.6571 Explore P: 0.4202\n",
      "Episode: 118 Total reward: 75.0 Training loss: 7.4171 Explore P: 0.4194\n",
      "Episode: 120 Total reward: 58.0 Training loss: 6.4510 Explore P: 0.4138\n",
      "Model Saved\n",
      "Episode: 121 Total reward: 57.0 Training loss: 3.0031 Explore P: 0.4124\n",
      "Episode: 122 Total reward: 93.0 Training loss: 1.9503 Explore P: 0.4121\n",
      "Episode: 123 Total reward: 69.0 Training loss: 9.9858 Explore P: 0.4110\n",
      "Episode: 125 Total reward: 34.0 Training loss: 7.0305 Explore P: 0.4049\n",
      "Model Saved\n",
      "Episode: 126 Total reward: 23.0 Training loss: 13.5334 Explore P: 0.4024\n",
      "Episode: 127 Total reward: 76.0 Training loss: 9.9130 Explore P: 0.4017\n",
      "Episode: 128 Total reward: 66.0 Training loss: 5.1086 Explore P: 0.4005\n",
      "Episode: 129 Total reward: 47.0 Training loss: 5.7886 Explore P: 0.3988\n",
      "Model Saved\n",
      "Episode: 131 Total reward: 53.0 Training loss: 2.6973 Explore P: 0.3934\n",
      "Episode: 132 Total reward: 32.0 Training loss: 2.5145 Explore P: 0.3912\n",
      "Episode: 134 Total reward: 74.0 Training loss: 4.5914 Explore P: 0.3866\n",
      "Episode: 135 Total reward: 60.0 Training loss: 5.2538 Explore P: 0.3852\n",
      "Model Saved\n",
      "Episode: 136 Total reward: 60.0 Training loss: 55.4046 Explore P: 0.3839\n",
      "Episode: 138 Total reward: 61.0 Training loss: 20.2419 Explore P: 0.3789\n",
      "Episode: 140 Total reward: 82.0 Training loss: 2.4278 Explore P: 0.3745\n",
      "Model Saved\n",
      "Episode: 141 Total reward: 48.0 Training loss: 7.9271 Explore P: 0.3729\n",
      "Episode: 142 Total reward: 78.0 Training loss: 2.8135 Explore P: 0.3721\n",
      "Episode: 143 Total reward: 52.0 Training loss: 3.7462 Explore P: 0.3707\n",
      "Model Saved\n",
      "Episode: 150 Total reward: 91.0 Training loss: 3.4575 Explore P: 0.3493\n",
      "Model Saved\n",
      "Episode: 152 Total reward: 71.0 Training loss: 4.1084 Explore P: 0.3451\n",
      "Episode: 153 Total reward: 95.0 Training loss: 6.3805 Explore P: 0.3449\n",
      "Episode: 154 Total reward: 95.0 Training loss: 4.8968 Explore P: 0.3447\n",
      "Episode: 155 Total reward: 29.0 Training loss: 2.1026 Explore P: 0.3427\n",
      "Model Saved\n",
      "Episode: 156 Total reward: 71.0 Training loss: 5.9126 Explore P: 0.3418\n",
      "Episode: 157 Total reward: 24.0 Training loss: 3.5430 Explore P: 0.3396\n",
      "Episode: 158 Total reward: 70.0 Training loss: 4.9837 Explore P: 0.3388\n",
      "Episode: 159 Total reward: 52.0 Training loss: 4.1730 Explore P: 0.3375\n",
      "Model Saved\n",
      "Episode: 161 Total reward: 76.0 Training loss: 2.2673 Explore P: 0.3336\n",
      "Episode: 162 Total reward: 73.0 Training loss: 9.8753 Explore P: 0.3328\n",
      "Episode: 163 Total reward: 53.0 Training loss: 9.8832 Explore P: 0.3314\n",
      "Model Saved\n",
      "Episode: 167 Total reward: 92.0 Training loss: 5.3043 Explore P: 0.3217\n",
      "Episode: 168 Total reward: 28.0 Training loss: 4.1124 Explore P: 0.3199\n",
      "Episode: 169 Total reward: 53.0 Training loss: 3.3446 Explore P: 0.3187\n",
      "Episode: 170 Total reward: 95.0 Training loss: 5.3684 Explore P: 0.3185\n",
      "Model Saved\n",
      "Episode: 171 Total reward: 90.0 Training loss: 5.9206 Explore P: 0.3182\n",
      "Episode: 172 Total reward: 74.0 Training loss: 5.5365 Explore P: 0.3175\n",
      "Episode: 173 Total reward: 70.0 Training loss: 9.5711 Explore P: 0.3167\n",
      "Episode: 174 Total reward: 95.0 Training loss: 4.2707 Explore P: 0.3165\n",
      "Episode: 175 Total reward: 52.0 Training loss: 1.8214 Explore P: 0.3152\n",
      "Model Saved\n",
      "Episode: 177 Total reward: 47.0 Training loss: 7.0910 Explore P: 0.3108\n",
      "Episode: 178 Total reward: 60.0 Training loss: 5.2348 Explore P: 0.3097\n",
      "Episode: 179 Total reward: 33.0 Training loss: 4.8965 Explore P: 0.3080\n",
      "Episode: 180 Total reward: 93.0 Training loss: 4.5452 Explore P: 0.3077\n",
      "Model Saved\n",
      "Episode: 181 Total reward: 34.0 Training loss: 4.1048 Explore P: 0.3059\n",
      "Episode: 182 Total reward: 16.0 Training loss: 2.2650 Explore P: 0.3038\n",
      "Episode: 183 Total reward: 85.0 Training loss: 3.7155 Explore P: 0.3034\n",
      "Episode: 184 Total reward: 50.0 Training loss: 3.0025 Explore P: 0.3022\n",
      "Episode: 185 Total reward: 41.0 Training loss: 4.4277 Explore P: 0.3007\n",
      "Model Saved\n",
      "Episode: 189 Total reward: 73.0 Training loss: 3.1472 Explore P: 0.2915\n",
      "Episode: 190 Total reward: 91.0 Training loss: 6.0242 Explore P: 0.2912\n",
      "Model Saved\n",
      "Episode: 191 Total reward: 75.0 Training loss: 7.5285 Explore P: 0.2906\n",
      "Episode: 192 Total reward: 41.0 Training loss: 9.5663 Explore P: 0.2892\n",
      "Episode: 193 Total reward: 42.0 Training loss: 3.2834 Explore P: 0.2878\n",
      "Episode: 194 Total reward: 64.0 Training loss: 2.4093 Explore P: 0.2869\n",
      "Model Saved\n",
      "Episode: 196 Total reward: 64.0 Training loss: 4.3638 Explore P: 0.2833\n",
      "Episode: 197 Total reward: 66.0 Training loss: 3.4478 Explore P: 0.2825\n",
      "Episode: 198 Total reward: 65.0 Training loss: 9.3395 Explore P: 0.2816\n",
      "Episode: 199 Total reward: 76.0 Training loss: 4.9989 Explore P: 0.2811\n",
      "Model Saved\n",
      "Episode: 201 Total reward: 60.0 Training loss: 5.2212 Explore P: 0.2774\n",
      "Episode: 203 Total reward: 92.0 Training loss: 8.1256 Explore P: 0.2745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 204 Total reward: 66.0 Training loss: 5.0154 Explore P: 0.2738\n",
      "Episode: 205 Total reward: 54.0 Training loss: 2.8044 Explore P: 0.2726\n",
      "Model Saved\n",
      "Episode: 206 Total reward: 73.0 Training loss: 2.6755 Explore P: 0.2720\n",
      "Episode: 208 Total reward: 20.0 Training loss: 4.6751 Explore P: 0.2677\n",
      "Episode: 209 Total reward: 61.0 Training loss: 4.3183 Explore P: 0.2668\n",
      "Episode: 210 Total reward: 47.0 Training loss: 1.5190 Explore P: 0.2657\n",
      "Model Saved\n",
      "Episode: 212 Total reward: 51.0 Training loss: 2.9795 Explore P: 0.2621\n",
      "Episode: 213 Total reward: 53.0 Training loss: 1.2311 Explore P: 0.2612\n",
      "Episode: 215 Total reward: 72.0 Training loss: 6.2749 Explore P: 0.2581\n",
      "Model Saved\n",
      "Episode: 216 Total reward: 87.0 Training loss: 5.0978 Explore P: 0.2577\n",
      "Episode: 217 Total reward: 74.0 Training loss: 4.4066 Explore P: 0.2572\n",
      "Episode: 219 Total reward: 34.0 Training loss: 5.4336 Explore P: 0.2534\n",
      "Episode: 220 Total reward: 93.0 Training loss: 4.4329 Explore P: 0.2532\n",
      "Model Saved\n",
      "Episode: 221 Total reward: 85.0 Training loss: 5.5265 Explore P: 0.2528\n",
      "Episode: 222 Total reward: 95.0 Training loss: 8.1922 Explore P: 0.2526\n",
      "Episode: 223 Total reward: 59.0 Training loss: 4.1535 Explore P: 0.2517\n",
      "Episode: 224 Total reward: 93.0 Training loss: 2.9694 Explore P: 0.2515\n",
      "Model Saved\n",
      "Episode: 227 Total reward: 95.0 Training loss: 3.3162 Explore P: 0.2466\n",
      "Episode: 228 Total reward: 25.0 Training loss: 3.1834 Explore P: 0.2452\n",
      "Episode: 229 Total reward: 94.0 Training loss: 3.5677 Explore P: 0.2450\n",
      "Episode: 230 Total reward: 66.0 Training loss: 4.3522 Explore P: 0.2443\n",
      "Model Saved\n",
      "Episode: 231 Total reward: 95.0 Training loss: 4.9387 Explore P: 0.2442\n",
      "Episode: 232 Total reward: 93.0 Training loss: 4.1163 Explore P: 0.2440\n",
      "Episode: 235 Total reward: 68.0 Training loss: 6.6036 Explore P: 0.2387\n",
      "Model Saved\n",
      "Episode: 236 Total reward: 92.0 Training loss: 8.2541 Explore P: 0.2385\n",
      "Episode: 237 Total reward: 66.0 Training loss: 4.4183 Explore P: 0.2378\n",
      "Episode: 238 Total reward: 37.0 Training loss: 2.8406 Explore P: 0.2367\n",
      "Episode: 240 Total reward: 49.0 Training loss: 4.6800 Explore P: 0.2335\n",
      "Model Saved\n",
      "Episode: 242 Total reward: 55.0 Training loss: 3.6657 Explore P: 0.2305\n",
      "Episode: 243 Total reward: 93.0 Training loss: 4.6380 Explore P: 0.2303\n",
      "Episode: 244 Total reward: 75.0 Training loss: 18.5224 Explore P: 0.2298\n",
      "Episode: 245 Total reward: 84.0 Training loss: 3.8719 Explore P: 0.2295\n",
      "Model Saved\n",
      "Episode: 246 Total reward: 92.0 Training loss: 5.6628 Explore P: 0.2293\n",
      "Episode: 247 Total reward: 65.0 Training loss: 2.6536 Explore P: 0.2286\n",
      "Episode: 248 Total reward: 57.0 Training loss: 12.1027 Explore P: 0.2277\n",
      "Episode: 249 Total reward: 67.0 Training loss: 4.0819 Explore P: 0.2271\n",
      "Episode: 250 Total reward: 49.0 Training loss: 16.1126 Explore P: 0.2262\n",
      "Model Saved\n",
      "Episode: 251 Total reward: -19.0 Training loss: 7.8889 Explore P: 0.2243\n",
      "Episode: 252 Total reward: 93.0 Training loss: 5.9031 Explore P: 0.2241\n",
      "Episode: 255 Total reward: 93.0 Training loss: 8.2420 Explore P: 0.2197\n",
      "Model Saved\n",
      "Episode: 256 Total reward: 45.0 Training loss: 8.3667 Explore P: 0.2187\n",
      "Episode: 257 Total reward: 95.0 Training loss: 5.6417 Explore P: 0.2186\n",
      "Episode: 258 Total reward: 93.0 Training loss: 8.3652 Explore P: 0.2184\n",
      "Episode: 259 Total reward: 93.0 Training loss: 7.4885 Explore P: 0.2183\n",
      "Episode: 260 Total reward: 74.0 Training loss: 8.2233 Explore P: 0.2178\n",
      "Model Saved\n",
      "Episode: 261 Total reward: 67.0 Training loss: 17.3623 Explore P: 0.2172\n",
      "Episode: 262 Total reward: 41.0 Training loss: 2.1064 Explore P: 0.2162\n",
      "Episode: 263 Total reward: 69.0 Training loss: 4.0496 Explore P: 0.2156\n",
      "Episode: 264 Total reward: 62.0 Training loss: 5.6162 Explore P: 0.2149\n",
      "Episode: 265 Total reward: 95.0 Training loss: 4.2433 Explore P: 0.2148\n",
      "Model Saved\n",
      "Episode: 266 Total reward: 48.0 Training loss: 4.2269 Explore P: 0.2139\n",
      "Episode: 267 Total reward: 90.0 Training loss: 3.6506 Explore P: 0.2137\n",
      "Episode: 268 Total reward: 52.0 Training loss: 2.6408 Explore P: 0.2129\n",
      "Episode: 269 Total reward: 66.0 Training loss: 16.3379 Explore P: 0.2123\n",
      "Episode: 270 Total reward: 40.0 Training loss: 2.8379 Explore P: 0.2113\n",
      "Model Saved\n",
      "Episode: 271 Total reward: 73.0 Training loss: 11.2652 Explore P: 0.2108\n",
      "Episode: 272 Total reward: 71.0 Training loss: 10.6736 Explore P: 0.2103\n",
      "Episode: 274 Total reward: 68.0 Training loss: 3.6112 Explore P: 0.2078\n",
      "Episode: 275 Total reward: 95.0 Training loss: 7.1008 Explore P: 0.2076\n",
      "Model Saved\n",
      "Episode: 276 Total reward: 52.0 Training loss: 4.4340 Explore P: 0.2069\n",
      "Episode: 278 Total reward: 92.0 Training loss: 4.5273 Explore P: 0.2047\n",
      "Episode: 279 Total reward: 71.0 Training loss: 3.2382 Explore P: 0.2042\n",
      "Episode: 280 Total reward: 72.0 Training loss: 5.1777 Explore P: 0.2038\n",
      "Model Saved\n",
      "Episode: 281 Total reward: 91.0 Training loss: 4.3409 Explore P: 0.2036\n",
      "Episode: 282 Total reward: 66.0 Training loss: 3.7216 Explore P: 0.2030\n",
      "Episode: 283 Total reward: 60.0 Training loss: 6.6007 Explore P: 0.2023\n",
      "Episode: 284 Total reward: 95.0 Training loss: 5.2296 Explore P: 0.2022\n",
      "Episode: 285 Total reward: 71.0 Training loss: 10.6905 Explore P: 0.2017\n",
      "Model Saved\n",
      "Episode: 286 Total reward: 47.0 Training loss: 2.7238 Explore P: 0.2009\n",
      "Episode: 287 Total reward: 66.0 Training loss: 4.6614 Explore P: 0.2003\n",
      "Episode: 288 Total reward: 93.0 Training loss: 4.1643 Explore P: 0.2001\n",
      "Episode: 289 Total reward: 90.0 Training loss: 3.9124 Explore P: 0.1999\n",
      "Episode: 290 Total reward: 37.0 Training loss: 3.3293 Explore P: 0.1988\n",
      "Model Saved\n",
      "Episode: 291 Total reward: 67.0 Training loss: 8.8122 Explore P: 0.1983\n",
      "Episode: 292 Total reward: 95.0 Training loss: 8.5293 Explore P: 0.1982\n",
      "Episode: 293 Total reward: -14.0 Training loss: 2.3272 Explore P: 0.1964\n",
      "Episode: 294 Total reward: 51.0 Training loss: 21.8834 Explore P: 0.1956\n",
      "Episode: 295 Total reward: 70.0 Training loss: 6.4288 Explore P: 0.1952\n",
      "Model Saved\n",
      "Episode: 296 Total reward: 72.0 Training loss: 3.7016 Explore P: 0.1947\n",
      "Episode: 297 Total reward: 25.0 Training loss: 7.6645 Explore P: 0.1936\n",
      "Episode: 298 Total reward: 57.0 Training loss: 5.4782 Explore P: 0.1929\n",
      "Episode: 299 Total reward: 68.0 Training loss: 8.1893 Explore P: 0.1924\n",
      "Episode: 300 Total reward: 88.0 Training loss: 3.2641 Explore P: 0.1921\n",
      "Model Saved\n",
      "Episode: 301 Total reward: 67.0 Training loss: 2.7649 Explore P: 0.1916\n",
      "Episode: 302 Total reward: 69.0 Training loss: 4.2479 Explore P: 0.1911\n",
      "Episode: 303 Total reward: 59.0 Training loss: 4.9842 Explore P: 0.1904\n",
      "Episode: 304 Total reward: 89.0 Training loss: 7.8342 Explore P: 0.1901\n",
      "Episode: 305 Total reward: 56.0 Training loss: 4.6104 Explore P: 0.1895\n",
      "Model Saved\n",
      "Episode: 306 Total reward: 93.0 Training loss: 5.1347 Explore P: 0.1894\n",
      "Episode: 307 Total reward: 65.0 Training loss: 9.1384 Explore P: 0.1888\n",
      "Episode: 308 Total reward: 74.0 Training loss: 4.5432 Explore P: 0.1884\n",
      "Episode: 309 Total reward: 90.0 Training loss: 4.2888 Explore P: 0.1882\n",
      "Episode: 310 Total reward: 75.0 Training loss: 7.8386 Explore P: 0.1878\n",
      "Model Saved\n",
      "Episode: 313 Total reward: 95.0 Training loss: 10.6705 Explore P: 0.1842\n",
      "Episode: 315 Total reward: 93.0 Training loss: 10.4007 Explore P: 0.1823\n",
      "Model Saved\n",
      "Episode: 316 Total reward: 66.0 Training loss: 3.7663 Explore P: 0.1818\n",
      "Episode: 318 Total reward: 76.0 Training loss: 3.9946 Explore P: 0.1798\n",
      "Episode: 319 Total reward: 76.0 Training loss: 4.2726 Explore P: 0.1794\n",
      "Model Saved\n",
      "Episode: 321 Total reward: 61.0 Training loss: 4.4879 Explore P: 0.1772\n",
      "Episode: 322 Total reward: 68.0 Training loss: 11.8705 Explore P: 0.1767\n",
      "Episode: 323 Total reward: 42.0 Training loss: 3.5469 Explore P: 0.1759\n",
      "Episode: 324 Total reward: 66.0 Training loss: 2.4141 Explore P: 0.1754\n",
      "Model Saved\n",
      "Episode: 326 Total reward: 67.0 Training loss: 8.6444 Explore P: 0.1733\n",
      "Episode: 328 Total reward: 63.0 Training loss: 3.9143 Explore P: 0.1711\n",
      "Episode: 329 Total reward: 95.0 Training loss: 3.1792 Explore P: 0.1710\n",
      "Episode: 330 Total reward: 72.0 Training loss: 3.2469 Explore P: 0.1706\n",
      "Model Saved\n",
      "Episode: 331 Total reward: 55.0 Training loss: 3.2867 Explore P: 0.1700\n",
      "Episode: 332 Total reward: 58.0 Training loss: 3.6526 Explore P: 0.1694\n",
      "Episode: 333 Total reward: 72.0 Training loss: 3.6225 Explore P: 0.1690\n",
      "Episode: 334 Total reward: 93.0 Training loss: 6.9975 Explore P: 0.1689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 335 Total reward: 84.0 Training loss: 8.1539 Explore P: 0.1686\n",
      "Model Saved\n",
      "Episode: 336 Total reward: 88.0 Training loss: 4.1085 Explore P: 0.1684\n",
      "Episode: 337 Total reward: 95.0 Training loss: 10.8246 Explore P: 0.1683\n",
      "Episode: 338 Total reward: 72.0 Training loss: 7.2764 Explore P: 0.1679\n",
      "Episode: 339 Total reward: 55.0 Training loss: 7.6965 Explore P: 0.1673\n",
      "Episode: 340 Total reward: 67.0 Training loss: 2.5963 Explore P: 0.1668\n",
      "Model Saved\n",
      "Episode: 341 Total reward: 93.0 Training loss: 10.0813 Explore P: 0.1667\n",
      "Episode: 342 Total reward: 66.0 Training loss: 6.0834 Explore P: 0.1662\n",
      "Episode: 343 Total reward: 45.0 Training loss: 10.1268 Explore P: 0.1655\n",
      "Episode: 344 Total reward: 67.0 Training loss: 5.4175 Explore P: 0.1650\n",
      "Episode: 345 Total reward: 71.0 Training loss: 4.8250 Explore P: 0.1647\n",
      "Model Saved\n",
      "Episode: 346 Total reward: 95.0 Training loss: 4.4446 Explore P: 0.1646\n",
      "Episode: 347 Total reward: 76.0 Training loss: 4.8295 Explore P: 0.1643\n",
      "Episode: 348 Total reward: 72.0 Training loss: 7.5204 Explore P: 0.1639\n",
      "Episode: 349 Total reward: 60.0 Training loss: 4.6011 Explore P: 0.1633\n",
      "Episode: 350 Total reward: 92.0 Training loss: 2.3911 Explore P: 0.1632\n",
      "Model Saved\n",
      "Episode: 351 Total reward: 23.0 Training loss: 4.4488 Explore P: 0.1622\n",
      "Episode: 352 Total reward: 48.0 Training loss: 4.7788 Explore P: 0.1615\n",
      "Episode: 353 Total reward: 95.0 Training loss: 13.1457 Explore P: 0.1614\n",
      "Episode: 354 Total reward: 50.0 Training loss: 7.7784 Explore P: 0.1608\n",
      "Episode: 355 Total reward: 55.0 Training loss: 5.2029 Explore P: 0.1602\n",
      "Model Saved\n",
      "Episode: 356 Total reward: 53.0 Training loss: 4.4162 Explore P: 0.1595\n",
      "Episode: 357 Total reward: 91.0 Training loss: 8.0389 Explore P: 0.1594\n",
      "Episode: 358 Total reward: 92.0 Training loss: 5.6098 Explore P: 0.1592\n",
      "Episode: 359 Total reward: 66.0 Training loss: 7.1900 Explore P: 0.1588\n",
      "Episode: 360 Total reward: 79.0 Training loss: 6.1181 Explore P: 0.1585\n",
      "Model Saved\n",
      "Episode: 361 Total reward: 95.0 Training loss: 6.1453 Explore P: 0.1584\n",
      "Episode: 362 Total reward: 47.0 Training loss: 5.0040 Explore P: 0.1577\n",
      "Episode: 363 Total reward: 64.0 Training loss: 4.6315 Explore P: 0.1573\n",
      "Episode: 364 Total reward: 66.0 Training loss: 8.2828 Explore P: 0.1568\n",
      "Episode: 365 Total reward: 69.0 Training loss: 5.8838 Explore P: 0.1564\n",
      "Model Saved\n",
      "Episode: 366 Total reward: 75.0 Training loss: 3.3839 Explore P: 0.1561\n",
      "Episode: 367 Total reward: 64.0 Training loss: 4.8959 Explore P: 0.1556\n",
      "Episode: 369 Total reward: 84.0 Training loss: 9.1385 Explore P: 0.1540\n",
      "Episode: 370 Total reward: 71.0 Training loss: 13.9391 Explore P: 0.1536\n",
      "Model Saved\n",
      "Episode: 371 Total reward: 66.0 Training loss: 6.2709 Explore P: 0.1532\n",
      "Episode: 372 Total reward: 87.0 Training loss: 10.9821 Explore P: 0.1530\n",
      "Episode: 373 Total reward: 95.0 Training loss: 3.2341 Explore P: 0.1529\n",
      "Episode: 374 Total reward: 94.0 Training loss: 5.5847 Explore P: 0.1528\n",
      "Episode: 375 Total reward: 50.0 Training loss: 6.5644 Explore P: 0.1522\n",
      "Model Saved\n",
      "Episode: 376 Total reward: 50.0 Training loss: 5.3178 Explore P: 0.1516\n",
      "Episode: 377 Total reward: 95.0 Training loss: 4.3060 Explore P: 0.1515\n",
      "Episode: 378 Total reward: 41.0 Training loss: 8.5151 Explore P: 0.1508\n",
      "Episode: 379 Total reward: 63.0 Training loss: 6.9222 Explore P: 0.1504\n",
      "Episode: 380 Total reward: 62.0 Training loss: 4.1381 Explore P: 0.1499\n",
      "Model Saved\n",
      "Episode: 381 Total reward: 47.0 Training loss: 10.7505 Explore P: 0.1493\n",
      "Episode: 382 Total reward: 45.0 Training loss: 5.3124 Explore P: 0.1486\n",
      "Episode: 383 Total reward: 16.0 Training loss: 9.1374 Explore P: 0.1476\n",
      "Episode: 384 Total reward: 95.0 Training loss: 6.1394 Explore P: 0.1475\n",
      "Model Saved\n",
      "Episode: 386 Total reward: 93.0 Training loss: 7.5961 Explore P: 0.1460\n",
      "Episode: 387 Total reward: 63.0 Training loss: 6.1166 Explore P: 0.1456\n",
      "Episode: 388 Total reward: 67.0 Training loss: 7.6638 Explore P: 0.1452\n",
      "Episode: 389 Total reward: 56.0 Training loss: 8.3133 Explore P: 0.1447\n",
      "Episode: 390 Total reward: 93.0 Training loss: 9.2152 Explore P: 0.1445\n",
      "Model Saved\n",
      "Episode: 391 Total reward: 75.0 Training loss: 6.5839 Explore P: 0.1443\n",
      "Episode: 392 Total reward: 47.0 Training loss: 4.5628 Explore P: 0.1437\n",
      "Episode: 394 Total reward: 72.0 Training loss: 5.6898 Explore P: 0.1420\n",
      "Episode: 395 Total reward: 48.0 Training loss: 6.1886 Explore P: 0.1415\n",
      "Model Saved\n",
      "Episode: 396 Total reward: 60.0 Training loss: 5.8963 Explore P: 0.1410\n",
      "Episode: 397 Total reward: 67.0 Training loss: 4.4498 Explore P: 0.1406\n",
      "Episode: 398 Total reward: 92.0 Training loss: 7.3589 Explore P: 0.1405\n",
      "Episode: 399 Total reward: 65.0 Training loss: 8.0768 Explore P: 0.1401\n",
      "Episode: 400 Total reward: 93.0 Training loss: 17.5166 Explore P: 0.1400\n",
      "Model Saved\n",
      "Episode: 401 Total reward: 54.0 Training loss: 7.1077 Explore P: 0.1394\n",
      "Episode: 402 Total reward: 66.0 Training loss: 10.0604 Explore P: 0.1391\n",
      "Episode: 403 Total reward: 66.0 Training loss: 5.6460 Explore P: 0.1387\n",
      "Episode: 405 Total reward: 90.0 Training loss: 7.4597 Explore P: 0.1372\n",
      "Model Saved\n",
      "Episode: 406 Total reward: 93.0 Training loss: 7.1149 Explore P: 0.1371\n",
      "Episode: 407 Total reward: 95.0 Training loss: 6.5121 Explore P: 0.1371\n",
      "Episode: 408 Total reward: 68.0 Training loss: 4.9105 Explore P: 0.1367\n",
      "Episode: 409 Total reward: 91.0 Training loss: 5.3756 Explore P: 0.1366\n",
      "Episode: 410 Total reward: 94.0 Training loss: 4.8986 Explore P: 0.1365\n",
      "Model Saved\n",
      "Episode: 411 Total reward: 94.0 Training loss: 6.0634 Explore P: 0.1364\n",
      "Episode: 412 Total reward: 56.0 Training loss: 4.6819 Explore P: 0.1359\n",
      "Episode: 413 Total reward: 73.0 Training loss: 42.4250 Explore P: 0.1356\n",
      "Episode: 414 Total reward: 91.0 Training loss: 7.0270 Explore P: 0.1355\n",
      "Episode: 415 Total reward: 56.0 Training loss: 6.7886 Explore P: 0.1350\n",
      "Model Saved\n",
      "Episode: 416 Total reward: 94.0 Training loss: 6.4879 Explore P: 0.1349\n",
      "Episode: 417 Total reward: 69.0 Training loss: 9.4001 Explore P: 0.1346\n",
      "Episode: 418 Total reward: 92.0 Training loss: 2.3512 Explore P: 0.1345\n",
      "Episode: 419 Total reward: 93.0 Training loss: 5.3365 Explore P: 0.1344\n",
      "Episode: 420 Total reward: 46.0 Training loss: 3.1093 Explore P: 0.1338\n",
      "Model Saved\n",
      "Episode: 421 Total reward: 57.0 Training loss: 4.5226 Explore P: 0.1333\n",
      "Episode: 422 Total reward: 73.0 Training loss: 5.1568 Explore P: 0.1330\n",
      "Episode: 423 Total reward: 91.0 Training loss: 2.5585 Explore P: 0.1329\n",
      "Episode: 424 Total reward: 46.0 Training loss: 5.9991 Explore P: 0.1324\n",
      "Episode: 425 Total reward: 57.0 Training loss: 7.2602 Explore P: 0.1319\n",
      "Model Saved\n",
      "Episode: 426 Total reward: 57.0 Training loss: 5.9737 Explore P: 0.1315\n",
      "Episode: 427 Total reward: 73.0 Training loss: 5.9018 Explore P: 0.1312\n",
      "Episode: 428 Total reward: 57.0 Training loss: 6.9517 Explore P: 0.1308\n",
      "Episode: 429 Total reward: 66.0 Training loss: 5.5247 Explore P: 0.1304\n",
      "Episode: 430 Total reward: 62.0 Training loss: 7.4893 Explore P: 0.1299\n",
      "Model Saved\n",
      "Episode: 431 Total reward: 44.0 Training loss: 10.7949 Explore P: 0.1294\n",
      "Episode: 432 Total reward: 93.0 Training loss: 5.6258 Explore P: 0.1293\n",
      "Episode: 434 Total reward: 59.0 Training loss: 6.0409 Explore P: 0.1277\n",
      "Episode: 435 Total reward: 41.0 Training loss: 5.6636 Explore P: 0.1271\n",
      "Model Saved\n",
      "Episode: 436 Total reward: 93.0 Training loss: 5.4317 Explore P: 0.1270\n",
      "Episode: 437 Total reward: 95.0 Training loss: 3.9183 Explore P: 0.1269\n",
      "Episode: 438 Total reward: 25.0 Training loss: 3.3927 Explore P: 0.1262\n",
      "Episode: 439 Total reward: 60.0 Training loss: 4.5041 Explore P: 0.1258\n",
      "Episode: 440 Total reward: 63.0 Training loss: 6.5976 Explore P: 0.1254\n",
      "Model Saved\n",
      "Episode: 441 Total reward: 78.0 Training loss: 5.7335 Explore P: 0.1251\n",
      "Episode: 442 Total reward: 66.0 Training loss: 3.0110 Explore P: 0.1248\n",
      "Episode: 443 Total reward: 87.0 Training loss: 3.9854 Explore P: 0.1246\n",
      "Episode: 444 Total reward: 63.0 Training loss: 13.3283 Explore P: 0.1242\n",
      "Episode: 445 Total reward: 58.0 Training loss: 10.0634 Explore P: 0.1238\n",
      "Model Saved\n",
      "Episode: 447 Total reward: 24.0 Training loss: 2.4130 Explore P: 0.1219\n",
      "Episode: 448 Total reward: 94.0 Training loss: 5.9062 Explore P: 0.1219\n",
      "Episode: 449 Total reward: 68.0 Training loss: 5.0643 Explore P: 0.1215\n",
      "Episode: 450 Total reward: 95.0 Training loss: 15.3795 Explore P: 0.1215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "Episode: 451 Total reward: 94.0 Training loss: 6.7339 Explore P: 0.1214\n",
      "Episode: 452 Total reward: 85.0 Training loss: 4.4719 Explore P: 0.1212\n",
      "Episode: 453 Total reward: 73.0 Training loss: 4.1629 Explore P: 0.1210\n",
      "Episode: 454 Total reward: 70.0 Training loss: 5.7178 Explore P: 0.1207\n",
      "Model Saved\n",
      "Episode: 456 Total reward: 54.0 Training loss: 15.6613 Explore P: 0.1191\n",
      "Episode: 457 Total reward: 94.0 Training loss: 11.2915 Explore P: 0.1190\n",
      "Episode: 458 Total reward: 64.0 Training loss: 4.3070 Explore P: 0.1187\n",
      "Episode: 459 Total reward: 73.0 Training loss: 5.5366 Explore P: 0.1184\n",
      "Episode: 460 Total reward: 49.0 Training loss: 3.6384 Explore P: 0.1180\n",
      "Model Saved\n",
      "Episode: 461 Total reward: 81.0 Training loss: 3.1348 Explore P: 0.1178\n",
      "Episode: 462 Total reward: 95.0 Training loss: 4.9702 Explore P: 0.1177\n",
      "Episode: 463 Total reward: 56.0 Training loss: 9.8976 Explore P: 0.1173\n",
      "Episode: 464 Total reward: 73.0 Training loss: 5.7947 Explore P: 0.1170\n",
      "Episode: 465 Total reward: 68.0 Training loss: 6.1633 Explore P: 0.1167\n",
      "Model Saved\n",
      "Episode: 466 Total reward: 30.0 Training loss: 25.2913 Explore P: 0.1161\n",
      "Episode: 467 Total reward: 31.0 Training loss: 4.1428 Explore P: 0.1155\n",
      "Episode: 468 Total reward: 93.0 Training loss: 7.3964 Explore P: 0.1154\n",
      "Episode: 469 Total reward: 66.0 Training loss: 6.1996 Explore P: 0.1151\n",
      "Episode: 470 Total reward: 75.0 Training loss: 3.5877 Explore P: 0.1149\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 73.0 Training loss: 3.3336 Explore P: 0.1146\n",
      "Episode: 472 Total reward: 61.0 Training loss: 26.2961 Explore P: 0.1143\n",
      "Episode: 473 Total reward: 94.0 Training loss: 5.6988 Explore P: 0.1142\n",
      "Episode: 474 Total reward: 95.0 Training loss: 3.5668 Explore P: 0.1141\n",
      "Episode: 475 Total reward: 58.0 Training loss: 9.9615 Explore P: 0.1137\n",
      "Model Saved\n",
      "Episode: 476 Total reward: 76.0 Training loss: 6.4992 Explore P: 0.1135\n",
      "Episode: 477 Total reward: 51.0 Training loss: 7.4762 Explore P: 0.1131\n",
      "Episode: 478 Total reward: 93.0 Training loss: 8.1853 Explore P: 0.1130\n",
      "Episode: 479 Total reward: 53.0 Training loss: 8.1369 Explore P: 0.1126\n",
      "Episode: 480 Total reward: 94.0 Training loss: 8.6834 Explore P: 0.1125\n",
      "Model Saved\n",
      "Episode: 481 Total reward: 54.0 Training loss: 8.1649 Explore P: 0.1121\n",
      "Episode: 482 Total reward: 77.0 Training loss: 2.9511 Explore P: 0.1119\n",
      "Episode: 483 Total reward: 71.0 Training loss: 8.2634 Explore P: 0.1116\n",
      "Episode: 484 Total reward: 94.0 Training loss: 11.6157 Explore P: 0.1116\n",
      "Episode: 485 Total reward: 54.0 Training loss: 5.2955 Explore P: 0.1112\n",
      "Model Saved\n",
      "Episode: 486 Total reward: 93.0 Training loss: 5.5709 Explore P: 0.1111\n",
      "Episode: 487 Total reward: 95.0 Training loss: 7.3183 Explore P: 0.1110\n",
      "Episode: 488 Total reward: 49.0 Training loss: 5.3988 Explore P: 0.1106\n",
      "Episode: 489 Total reward: 93.0 Training loss: 3.9815 Explore P: 0.1105\n",
      "Episode: 490 Total reward: 71.0 Training loss: 7.9366 Explore P: 0.1103\n",
      "Model Saved\n",
      "Episode: 491 Total reward: 62.0 Training loss: 6.8263 Explore P: 0.1099\n",
      "Episode: 492 Total reward: 66.0 Training loss: 3.5952 Explore P: 0.1096\n",
      "Episode: 493 Total reward: 87.0 Training loss: 38.2062 Explore P: 0.1095\n",
      "Episode: 494 Total reward: 61.0 Training loss: 6.8465 Explore P: 0.1091\n",
      "Episode: 495 Total reward: 95.0 Training loss: 6.1871 Explore P: 0.1091\n",
      "Model Saved\n",
      "Episode: 496 Total reward: 93.0 Training loss: 4.4654 Explore P: 0.1090\n",
      "Episode: 497 Total reward: 90.0 Training loss: 9.6807 Explore P: 0.1089\n",
      "Episode: 498 Total reward: 34.0 Training loss: 17.9137 Explore P: 0.1083\n",
      "Episode: 499 Total reward: 61.0 Training loss: 4.8840 Explore P: 0.1080\n"
     ]
    }
   ],
   "source": [
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \n",
    "        decay_step = 0\n",
    "\n",
    "        # Init the game\n",
    "        game.init()\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step +=1\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "\n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Training loss: {:.4f}'.format(loss),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\n",
    "\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                    \n",
    "\n",
    "                    # Add experience to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    # st+1 is now our current state\n",
    "                    state = next_state\n",
    "\n",
    "\n",
    "                ### LEARNING PART            \n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                 # Get Q values for next_state \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                               DQNetwork.target_Q: targets_mb,\n",
    "                                               DQNetwork.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                   DQNetwork.target_Q: targets_mb,\n",
    "                                                   DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "\n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Score:  45.0\n",
      "Score:  71.0\n",
      "Score:  60.0\n",
      "Score:  63.0\n",
      "Score:  61.0\n",
      "Score:  87.0\n",
      "Score:  95.0\n",
      "Score:  95.0\n",
      "Score:  85.0\n",
      "Score:  90.0\n",
      "Score:  85.0\n",
      "Score:  71.0\n",
      "Score:  85.0\n",
      "Score:  67.0\n",
      "Score:  64.0\n",
      "Score:  67.0\n",
      "Score:  88.0\n",
      "Score:  95.0\n",
      "Score:  93.0\n",
      "Score:  71.0\n",
      "Score:  67.0\n",
      "Score:  51.0\n",
      "Score:  61.0\n",
      "Score:  47.0\n",
      "Score:  92.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = create_environment()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(25):\n",
    "        \n",
    "        done = False\n",
    "        \n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "            \n",
    "        while not game.is_episode_finished():\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[int(choice)]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "            if done:\n",
    "                break  \n",
    "                \n",
    "            else:\n",
    "#                 print(\"else\")\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "                state = next_state\n",
    "                time.sleep(0.05)\n",
    "                \n",
    "        score = game.get_total_reward()\n",
    "        print(\"Score: \", score)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
